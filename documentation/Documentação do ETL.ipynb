{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8d18e7",
   "metadata": {},
   "source": [
    "# **Desafio - Dev Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e822ab16",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Com base nas instruções fornecidas para a execução do desafio, foram realizados os seguintes passos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff34910",
   "metadata": {},
   "source": [
    "## **Avaliação dos dados**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d636e51",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Para uma manipulação mais assertiva dos dados, foi feita uma primeira avaliação dos dados contidos dentro dos arquivos disponibilizados, pois dessa forma, foi possível criar um mapemamento de como o pipeline do ETL iria ocorrer.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f6304",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Após a abertura de todos os arquivos, chegou-se as seguites conclusões:\n",
    "- Os arquivos Adult.data e Adult.test não possuem cabeçalho;\n",
    "- O arquivo Adult.test possui na primeira linha ***1x3 Cross validator*** dando a entender que na verdade esses dois arquivos são o resultado de um ***Data Splitting*** e que os dois arquivos foram utilizados para treinamento de um modelo de Machine Learning. Sendo um arquivo para a validação e o outro para o treinamento do modelo;\n",
    "- No arquivo Description estão os cabeçalhos da tabela;\n",
    "- No arquivo Description contém o tipo dos dados: categóricos e numéricos;\n",
    "- Ainda no arquivo Description, existe uma última coluna com o nome ***class***, termo utilizado em modelos de ML, que fazem referência a uma coluna utilizada para alguma previsão, mais uma pista que os dados foram utilizados na criação de um modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d5b16",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Antes do início do processo de ETL, foi feita uma varredura nos dados para identificar se existia algum registro nas colunas que não corresponde com a categoria da coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2584ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adult_data = pd.read_csv('data/Adult.data', header=None, sep=',')\n",
    "for i in range(15):\n",
    "    print (adult_data[i].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf55dea",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, foi possível identificar ***letras*** em colunas numéricas e ***?*** em colunas categóricas.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Finalizada essa etapa de avaliação, começou-se o processo de elaboração do script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7b18f",
   "metadata": {},
   "source": [
    "## **ETL > Extração**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2003b4",
   "metadata": {},
   "source": [
    "Para a extração, foi criada uma função que recebe por parâmetro o número de linhas a serem processadas (1630) e as linhas do arquivo que devem ser desconsideradas, para não existir reprocessamento nos ciclos do script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61299600",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_data, adult_test = load_adult_datasets(\n",
    "        first_n_lines = None if not auto else num_lines_to_process,\n",
    "        skip_lines = (lines_processed_data, lines_processed_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158db585",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;No início da função, é feita a extração do nome das colunas e a categoria de dado de cada uma delas do arquivo Description.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;É lido o arquivo inteiro e a string ajustada de forma que cada nome de coluna fique numa única linha após o ***Attribute type:***. Dessa forma, foi possível extrair o nome de cada uma delas e já substituir o - por _ para uma melhor utilização no banco de dados.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Além disso, a categoria da coluna também foi extraída com o auxílio do termo ***continuous*** que indicava um valor numérico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540387ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_header_info():\n",
    "    headers = []\n",
    "    data_type = []\n",
    "    lines = \"\"\n",
    "\n",
    "    with open('data/Description', 'r') as f:\n",
    "        lines = f.read()\n",
    "\n",
    "    lines = lines.replace(': \\n', ':').split('\\n')\n",
    "\n",
    "    attributes_index_start = 0\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        if \"Attribute type\" in lines[i]:\n",
    "            attributes_index_start = i + 1\n",
    "    \n",
    "    for i in range(attributes_index_start, len(lines)):\n",
    "        if lines[i].strip() == '':\n",
    "            break\n",
    "\n",
    "        header_metadata = lines[i].split(':')\n",
    "        headers.append(header_metadata[0].strip().replace('-', '_'))\n",
    "\n",
    "        is_data_type_numeric = 'continuous' in header_metadata[1]\n",
    "        data_type.append(is_data_type_numeric)\n",
    "\n",
    "    return headers, data_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f31ab1a",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Com os cabeçalhos e categorias das colunas, então é criado o dataframe do bloco de linhas que está sendo extraído do csv original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc85ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    adult_data = pd.read_csv('data/Adult.data', header=None, sep=',', skiprows=skip_lines[0], nrows=first_n_lines)\n",
    "except pd.errors.EmptyDataError:\n",
    "    adult_data = pd.DataFrame(columns=headers)\n",
    "\n",
    "try:\n",
    "    adult_test = pd.read_csv('data/Adult.test', header=None, sep=',', skiprows=1 + skip_lines[1], nrows=first_n_lines)\n",
    "except pd.errors.EmptyDataError:\n",
    "    adult_test = pd.DataFrame(columns=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73126fb2",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Como os dois arquivos csv contém a mesma estrutura, ambos foram tratados da mesma forma, para evitar duplicação de código.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Então são adicionada as colunas ao dataframe. Se o dataframe não estiver vazio, ele itera pelas colunas, já fazendo um primeiro tratamento.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Caso a categoria da coluna for categórica, ele substitui os ***?*** por nulo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa31a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [adult_data, adult_test]:\n",
    "    data.columns = headers\n",
    "\n",
    "    if len(data) > 0:\n",
    "        for i in range(len(headers)):\n",
    "            if data_type[i]:\n",
    "                data[headers[i]] = data[headers[i]].parallel_apply(lambda x: try_convert_numeric(x))\n",
    "            else:\n",
    "                data[headers[i]] = data[headers[i]].astype(str)\n",
    "                data[headers[i]] = data[headers[i]].parallel_apply(lambda x: None if x.strip() == '?' else x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32838462",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Neste caso, foi utilizado o ***parallel_apply()*** no lugar do ***apply()*** do Pandas, por ser multithread, otimizando a execução do script. \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Se a categoria da coluna for numérica, ele valida se é um número válido, se não for, ele retorna nulo, com o auxílio de uma função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92621aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_convert_numeric(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdfc982",
   "metadata": {},
   "source": [
    "## **ETL > Transformação**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7137e6f",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Dando continuidade a etapa anterior, nessa etapa, é feita a normalização dos dados e também o tratamento aos dados nulos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [(adult_data, \"AdultData\"), (adult_test, \"AdultTest\")]:\n",
    "    dataset, name = data\n",
    "\n",
    "    dataset = handle_missing_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1312b2",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;É criado um dicionário com a quantidade de registros nulos para cada coluna. Caso existam valores nulos na coluna, então é feito o tratamento:\n",
    "- Se a coluna for numerica, é feita uma média com todos os valores da coluna e esse valor é inserido no lugar do valor nulo;\n",
    "- Se for categórica, então é pego o valor com mais ocorrências na coluna e este adicionado no lugar do valor nulo;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fe3f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_data(data):\n",
    "    missing_data = dict(data.isnull().sum())\n",
    "\n",
    "    for column, missing_count in missing_data.items():\n",
    "        if missing_count > 0:\n",
    "            print(f\"{column} has {missing_count} missing values.\")\n",
    "\n",
    "            if data[column].dtype == 'int64' or data[column].dtype == 'float64':\n",
    "                print(f\"{column} has missing values. Replacing with mean.\\n\")\n",
    "                data[column].fillna(data[column].mean(), inplace=True)\n",
    "            else:\n",
    "                print(f\"{column} has missing values. Replacing with mode.\\n\")\n",
    "                data[column].fillna(data[column].mode()[0], inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a56ea3",
   "metadata": {},
   "source": [
    "## **ETL > Carga**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd5b3c7",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Por fim, os dados são inseridos no banco de dados SQLite. Primeiro é feita uma validação se a tabela já existe no banco. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8cda50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(table_name, dataset):\n",
    "\n",
    "    check_sqlite_table(table_name, dataset)\n",
    "\n",
    "    dataset.to_sql(\n",
    "        name=table_name,\n",
    "        if_exists='append',\n",
    "        index=False,\n",
    "        method='multi', \n",
    "        chunksize=200,\n",
    "        con=load_sqlite_database()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbf5d8",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Caso não exista, é criada respeitando a categoria de dados que cada coluna contém. Também foi adicionada uma coluna **id** para ser a ***PK*** da tabela. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397d8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sqlite_table(table_name, base_dataset):\n",
    "    if not sqlalchemy.inspect(load_sqlite_database()).has_table(table_name):\n",
    "        field_and_types = \"\"\n",
    "\n",
    "        for column_name, dtype in zip(base_dataset.columns, base_dataset.dtypes):\n",
    "\n",
    "            if dtype == 'float64' or dtype == 'int64':\n",
    "                field_and_types += f\"{column_name} REAL,\"\n",
    "            else: \n",
    "                field_and_types += f\"{column_name} TEXT,\"\n",
    "\n",
    "        conn = load_sqlite_database().connect()\n",
    "        trans = conn.begin()\n",
    "\n",
    "        conn.execute(f\"\"\"\n",
    "            CREATE TABLE {table_name}\n",
    "                (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                {field_and_types[:-1]}); \n",
    "        \"\"\")\n",
    "\n",
    "        trans.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ecef06",
   "metadata": {},
   "source": [
    "## **Agendamento**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1577bee",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Foi utilizado o  ***Crontab*** para realizar o agendamento do script. Como essa ferramenta não permite agendamentos em segundos, então foi feita uma lógica para que o script rode internamente a cada 10 segundos, porém sendo agendado a cada ***um minuto*** pelo ***Crontab***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a284e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pipeline_main(periodic) \n",
    "\n",
    "for i in range(5):\n",
    "    while (time.time() - start_time) <= 10:                \n",
    "        time.sleep(0.02)\n",
    "\n",
    "    pipeline_main(periodic)\n",
    "\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4d0403",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;O tempo no início da primeira execução do pipeline é gravado numa variável, e após finalizar esta execução, o script faz uma série de iterações, onde o script aguarda ***10 segundos*** para rodar novamente um pipeline.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Antes de finalizar cada iteração, a variável com o tempo é resetada, para garantir que ele sempre consiga esperar os 10 segundos em todas as iterações subsequentes.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, o pipeline é acionado 6 vezes (a cada 10 segundos) a cada minuto, e o agendador é acionado a cada 1 minuto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbacda9",
   "metadata": {},
   "source": [
    "## **Processamento em blocos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05332d1",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Para que o pipeline processasse apenas 1630 registros por vez, foi utilizado o ***pickle***, pois com ele, é possível gravar na pasta raiz do script, quais os registros já foram processados a cada pipeline.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Para esse desafio em especial, assumiu-se que os dados da tabela não são alterados depois que eles vão para o banco de dados.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Duas funções foram elaboradas:\n",
    "- ***get_processed_lines_read()***: Que retorna as últimas linhas processadas no arquivo csv original;\n",
    "- ***set_processed_lines_read()***: Que adiciona as últimas linhas processadas em variáveis globais. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44867c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('processed_lines.pkl'):\n",
    "    processed_lines = pickle.load(open('processed_lines.pkl', 'rb'))\n",
    "   \n",
    "    set_processed_lines_read(processed_lines['data'], processed_lines['test'])\n",
    "\n",
    "lines_processed_data, lines_processed_test = get_processed_lines_read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9c64b",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;No inicio do pipeline, é feita a validação se já existe um arquivo ***pickle***, se sim, ele é lido e os valores inseridos nele são inseridos nas variáveis globais do script.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Caso ele não exista, então é utilizado o valor padrão dessas variáveis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e73d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_processed_data = 0\n",
    "lines_processed_test = 0\n",
    "\n",
    "def get_processed_lines_read():\n",
    "    return lines_processed_data, lines_processed_test\n",
    "\n",
    "def set_processed_lines_read(crd, crt):\n",
    "    global lines_processed_data\n",
    "    global lines_processed_test\n",
    "\n",
    "    lines_processed_data, lines_processed_test = crd, crt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c124b0",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Somente depois de pegar esses valores das variáveis, que começa a fase de ***extração*** do ETL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_processed_data, lines_processed_test = get_processed_lines_read()\n",
    "\n",
    "    # ETL Phase 1 - Extract >> Get datasets \n",
    "    adult_data, adult_test = load_adult_datasets(\n",
    "        first_n_lines = None if not auto else num_lines_to_process,\n",
    "        skip_lines = (lines_processed_data, lines_processed_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9890ef1",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Quando o ETL é concluído, ou seja, após a fase de ***carga***, os valores contidos nas variáveis globais são inseridos novamente no arquivo ***pickle***, mantendo assim o registro de quais foram as últimas linhas processadas do arquivo csv original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee02cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_processed_data, lines_processed_test = get_processed_lines_read()\n",
    "pickle.dump({ 'data': lines_processed_data, 'test': lines_processed_test }, open('processed_lines.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31035140",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Dessa forma, existe um controle de quais registros foram processados, fazendo com que a mesma linha do csv não seja processada mais de uma vez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47138d6",
   "metadata": {},
   "source": [
    "## **EXTRA I - Análise Exploratória de Dados**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43649a3",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Foi inserido no script, uma opção para rodar uma análise exploratória dos dados. Ela vem logo após a fase ***transformação*** do ETL, pois para ela funcionar corretamente, todos o dataframe não pode conter registros que não condizem com a categoria das colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce2f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eda = True\n",
    "\n",
    "if run_eda and not auto:\n",
    "    run_exploratory_data_analysis(dataset, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34ed3be",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;A biblioteca utilizada para essa análise foi a ***sweetviz***.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Após a conclusão da análise, um arquivo ***.html *** é criado e salvo para auxiliar na extração de insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz as sv\n",
    "import os\n",
    "\n",
    "def run_exploratory_data_analysis(data, name):\n",
    "\n",
    "    if not os.path.exists('EDA'):\n",
    "        os.makedirs('EDA')\n",
    "\n",
    "    data_report = sv.analyze(data, pairwise_analysis=\"on\")\n",
    "    data_report.show_html(filepath=f\"EDA/{name}.html\", open_browser=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f00de04",
   "metadata": {},
   "source": [
    "## **EXTRA II - Modelo de Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb1dc9",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Assim como o EDA, também foi elaborado um modelo de Machine Learning para criar previsão de uma determinada coluna com base nos dados fornecidos.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Além da previsão, ainda são extraídos o grau de importância de cada coluna para aquela tomada de decisão. Mais uma ferramenta para auxiliar na extração de insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_insights = True\n",
    "if get_insights and not auto:\n",
    "    get_importance_of_attributes_to_class(adult_data, 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d6c783",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Para esse modelo, foi utilizada a coluna class, e a previsão a ser feita é:   \n",
    "***Quanto um indivíduo faria no ano com base em seus dados estatísticos: Mais ou menos de $50.000,00?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b9e44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_importance_of_attributes_to_class(dataset, _class):\n",
    "    x, y = dataset.drop(_class, axis=1), dataset[_class]\n",
    "    clf = RandomForestClassifier()\n",
    "    enc = OrdinalEncoder()\n",
    "    y = enc.fit_transform(y.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "    original_column_names = list(x.columns)\n",
    "    original_column_names = sorted(original_column_names, reverse=True)\n",
    "\n",
    "    x = pd.get_dummies(x)\n",
    "\n",
    "    clf.fit(x,y)\n",
    "    feat_importances = get_feature_importance(clf, original_column_names)\n",
    "\n",
    "    print('Importances of attributes to how much money an adult would make:')\n",
    "    for f, i in feat_importances.items():\n",
    "        print(f'{f} : {round(i*100, 2)}%')\n",
    "\n",
    "    feat_importances[_class] = 'Prediction Label'\n",
    "    importance = pd.DataFrame([feat_importances])\n",
    "    insert_data(\"feature_importance\", importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff92e02",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Foi utilizado um ***OrdinalEncoder*** para converter as colunas categóricas em colunas binárias.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Também foi extraído o nome original do dataframe, pois quando as colunas são divididas, essas novas colunas contêm o nome original da coluna como prefixo do seu nome.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Após o treinamento do modelo, é chamada a função que pega o grau de importância de cada coluna para aquela tomada de decisão. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca1161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(model, original_column_name):\n",
    "    feature_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                        index=model.feature_names_in_,\n",
    "                                        columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "    importances = { feature : 0 for feature in original_column_name }\n",
    "\n",
    "    for index, row in feature_importances.iterrows():\n",
    "        feature_name = index.strip()\n",
    "        curr_importance = row[0]\n",
    "        \n",
    "        for feature in original_column_name:\n",
    "            if feature.startswith(feature_name):\n",
    "                importances[feature] += curr_importance\n",
    "                break\n",
    "    \n",
    "    return importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e704c1",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Como as colunas foram divididas, então existe a necessidade de se somar o grau de importância de todas as colunas que têm a mesma origem. Por esse motivo foi salvo o nome original de todas as colunas.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;A função faz a soma dessas colunas com o mesmo prefixo e retorna o valor final de importância para cada coluna original do dataframe.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Uma nova tabela é criada no banco, guardando as informações referentes ao grau de importância para cada coluna prevista no modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c393c645",
   "metadata": {},
   "source": [
    "## **>>> IMPORTANTE - Informações adicionais**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba0a475",
   "metadata": {},
   "source": [
    "### **Observação 1 - Arquivos Crontab** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc37ca",
   "metadata": {},
   "source": [
    "Foram inseridos na pasta ***scheduler*** dois arquivos:\n",
    "- ***census_bureau_pipeline.sh:*** Arquivo a ser lido pelo ***Crontab*** do Linux;\n",
    "- ***crontab:*** Script usado no linux para iniciar o Pipeline;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18edfaae",
   "metadata": {},
   "source": [
    "### **Observação 2 - Rodando Pipeline** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de78d1",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Existem duas formas de se rodar o pipeline:\n",
    "- De forma automática pelo ***Crontab***;\n",
    "- De forma manual pelo terminal;\n",
    "C&nbsp;&nbsp;&nbsp;&nbsp;Como o pipeline do ETL não necessita do EDA, nem do modelo de ML para funcionar, a ativação dessas duas opções são customizáveis no próprio script.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Basta que as variáveis (**run_eda** e **get_insights**) estejam como **True** e a variável **auto** como **False**.\n",
    "O script reconhece de forma automática quando o script é acionado de forma automática pelo ***Crontab*** porque ele inicia o script com o argumento *periodic*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249a528",
   "metadata": {},
   "source": [
    "### **Observação 3 - Requirements** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36035c44",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Todas as bibliotecas utilizadas na elaboração do script se encontram no arquivo de texto ***requirements.txt***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
